{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1cAxdvd61e"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
        "\n",
        "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/notebooks/blob/main/notebooks/how-to-use-ultralytics-yolo-with-openai-for-number-plate-recognition.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  \n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
        "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
        "  \n",
        "  Welcome to the Ultralytics YOLO11 üöÄ notebook! <a href=\"https://github.com/ultralytics/ultralytics\">YOLO11</a> is the latest version of the YOLO (You Only Look Once) AI models developed by <a href=\"https://ultralytics.com\">Ultralytics</a>. We hope that the resources in this notebook will help you get the most out of YOLO11. Please browse the YOLO11 <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64B195mdjxQd"
      },
      "source": [
        "# Automatic Number Plate Recognition using Ultralytics YOLO11 + OpenAI `gpt-4o-mini`\n",
        "\n",
        "This notebook provides a comprehensive guide to implementing automatic number plate recognition (ANPR) using the YOLO11 model in combination with `gpt-4o-mini`.\n",
        "\n",
        "## What is Automatic Number Plate Recognition (ANPR)?\n",
        "Automatic number plate recognition is a technology designed to identify and extract vehicle number plate information from images or videos. By leveraging the powerful capabilities of Ultralytics YOLO11 for object detection and OpenAI `gpt-4o-mini` for text recognition, ANPR becomes an efficient solution for automating vehicle identification tasks.\n",
        "\n",
        "## Why Use YOLO11 + GPT-4o-Mini for ANPR?\n",
        "\n",
        "- Accurate Detection: YOLO11‚Äôs object detection capabilities ensure precise localization of license plates in various conditions, such as low light or high-speed movement.\n",
        "\n",
        "- Seamless Text Recognition: With `gpt-4o-mini`, extracted license plate regions are processed to recognize alphanumeric text accurately, even with variations in font, angle, or clarity.\n",
        "\n",
        "- Real-Time Processing: The integration allows for real-time vehicle monitoring, making it ideal for applications in traffic management, parking systems, and security surveillance.üöó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o68Sg1oOeZm2"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dSwz_uOReMI",
        "outputId": "8d020f28-71eb-41d5-b954-3899b490c8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.65 üöÄ Python-3.11.11 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 30.8/112.6 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "import cv2\n",
        "import base64\n",
        "\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.plotting import Annotator, colors\n",
        "from ultralytics.utils.downloads import safe_download\n",
        "\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Video and Model File\n",
        "\n",
        "Download the sample video we‚Äôll use for processing. If you prefer to use your own video file, downloading the sample is not necessary.  \n",
        "\n",
        "‚úÖ We‚Äôll download a license plate detection model `anpr-demo-model.pt` trained on a small dataset, designed to detect license plates in the sample video. You are welcome to use your own custom models as well.  \n",
        "\n",
        "‚ö†Ô∏è Note: This license plate detection model `anpr-demo-model.pt` is intended solely for proof of concept (POC) purposes and may only work with `anpr-demo-video.mp4`."
      ],
      "metadata": {
        "id": "IIZw8IJOk3Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the sample video file\n",
        "safe_download(\"https://github.com/ultralytics/assets/releases/download/v0.0.0/anpr-demo-video.mp4\")\n",
        "\n",
        "# download the sample model file\n",
        "safe_download(\"https://github.com/ultralytics/assets/releases/download/v0.0.0/anpr-demo-model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkajxl6nlObU",
        "outputId": "38d4aa96-9750-42c3-f609-3425be5ad4c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/anpr-demo-video.mp4 to 'anpr-demo-video.mp4'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.52M/9.52M [00:00<00:00, 106MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/anpr-demo-model.pt to 'anpr-demo-model.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.20M/5.20M [00:00<00:00, 65.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpKaC03TZDlr"
      },
      "source": [
        "### Read the Video and Model File\n",
        "\n",
        "You can either read the video file directly or stream content from an RTSP (Real-Time Streaming Protocol) source, offering flexible video input options to meet your requirements.\n",
        "\n",
        "‚úÖ By default, we will use the demo video `anpr-demo-video.mp4` downloaded in the previous step. Additionally, we‚Äôll set up the video writer to manage the output video processing.\n",
        "\n",
        "‚úÖ The model `anpr-demo-model.pt` will also be initialized in memory to handle the processing. You can also use your own model for license plate detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2daZuXgaZDFH"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(\"/content/anpr-demo-video.mp4\")\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "# Video writer\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "video_writer = cv2.VideoWriter(\"anpr-output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "# Load the Ultralytics YOLO license plate detection model\n",
        "model = YOLO(\"/content/anpr-demo-model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img align=\"left\" src=\"https://github.com/user-attachments/assets/101152c3-25ba-48a7-9916-c32c3be5857e\" height=\"640\">"
      ],
      "metadata": {
        "id": "e0QOXawB2-Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure OpenAI Client\n",
        "\n",
        "It‚Äôs time to configure the OpenAI client that will accept `base64` image data, extract the number plate text, and return it as a response. The configuration allows you to adjust the content sent to the OpenAI model, and you can also choose different models, such as `gpt-4o`.\n",
        "\n",
        "‚ö†Ô∏è For the API key, i.e., `OpenAI(api_key=\"api_key\")`, you'll need to visit the [OpenAI API key settings page](https://platform.openai.com/settings/organization/api-keys) and generate an API key for use.\n",
        "\n",
        "The `extract_text` function is designed to process a base64-encoded image containing a vehicle's number plate.\n",
        "\n",
        "‚úÖ It sends the image data to the OpenAI client using the `gpt-4o-mini` model and requests the extraction of only the license plate text.\n",
        "\n",
        "‚úÖ If the text cannot be extracted, the function ensures a response of None. -\n",
        "\n",
        "‚úÖ Additionally, it filters out any text written near the license plate to provide accurate results.\n"
      ],
      "metadata": {
        "id": "I8MS12NZuooP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=\"api_key_string\")\n",
        "\n",
        "# Define the text prompt\n",
        "prompt = \"\"\"\n",
        "Can you extract the vehicle number plate text inside the image?\n",
        "If you are not able to extract text, please respond with None.  # Fallback instruction\n",
        "Only output text, please.  # Ensure no extra formatting\n",
        "If any text character is not from the English language, replace it with a dot (.)  # Handle non-English characters\n",
        "\"\"\"\n",
        "\n",
        "def extract_text(base64_encoded_data):\n",
        "  response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt\n",
        "\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_encoded_data}\"},\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "      )\n",
        "  return response"
      ],
      "metadata": {
        "id": "RPXlG6IKticF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7VkxQ2aeg7k"
      },
      "source": [
        "### Process Video Frames\n",
        "\n",
        "In this step, we will process video frames to detect objects, crop regions with padding, and extract license plate text using an OpenAI model. Here's how it works:\n",
        "\n",
        "‚úÖ Frames are read using OpenCV, and objects are detected using the YOLO11 model.\n",
        "Bounding boxes are adjusted with padding to crop regions of interest while ensuring proper boundaries.\n",
        "\n",
        "‚úÖ Cropped regions are encoded in base64 and sent to the `extract_text` function, which uses OpenAI‚Äôs model to retrieve license plate text.\n",
        "\n",
        "‚úÖ The extracted text is added as a label to the bounding box on the video frame.\n",
        "\n",
        "‚úÖ Processed frames are saved to an output video file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cx-u59HQdu2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f774009e-556b-496c-cad7-11db5d970941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x384 4 plates, 10.4ms\n",
            "Speed: 3.2ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Extracted text: 5379MX-4\n",
            "Extracted text: BY 3066\n",
            "Extracted text: 8864 KA-4\n",
            "Extracted text: None\n",
            "\n",
            "0: 640x384 4 plates, 28.4ms\n",
            "Speed: 4.2ms preprocess, 28.4ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Extracted text: None\n",
            "Extracted text: 5379MX-4\n",
            "Extracted text: B 6964 KA-6\n",
            "Extracted text: None\n",
            "\n",
            "0: 640x384 4 plates, 13.9ms\n",
            "Speed: 5.0ms preprocess, 13.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Extracted text: 5379MX-4\n",
            "Extracted text: BY 3066 V\n",
            "Extracted text: 8884 KA 6\n",
            "Extracted text: None\n",
            "\n",
            "0: 640x384 4 plates, 15.8ms\n",
            "Speed: 4.4ms preprocess, 15.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Extracted text: 3066M\n",
            "Extracted text: BY 5379MX-4\n",
            "Extracted text: B0064 K.A-4\n",
            "Extracted text: None\n"
          ]
        }
      ],
      "source": [
        "padding = 10  # Adjust the padding value as needed\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, im0 = cap.read()\n",
        "\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    results = model.predict(im0)[0].boxes\n",
        "    boxes = results.xyxy.cpu()\n",
        "    clss = results.cls.cpu()\n",
        "\n",
        "    ann = Annotator(im0, line_width=3)\n",
        "\n",
        "    for cls, box, in zip(clss, boxes):\n",
        "\n",
        "      height, width, _ = im0.shape  # Get the dimensions of the original image\n",
        "\n",
        "      # Calculate padded coordinates\n",
        "      x1 = max(int(box[0]) - padding, 0)\n",
        "      y1 = max(int(box[1]) - padding, 0)\n",
        "      x2 = min(int(box[2]) + padding, width)\n",
        "      y2 = min(int(box[3]) + padding, height)\n",
        "\n",
        "\n",
        "      # Crop the object with padding and encode the numpy array to base64 format.\n",
        "      base64_im0 = base64.b64encode(cv2.imencode('.jpg', im0[y1:y2, x1:x2])[1]).decode('utf-8')\n",
        "\n",
        "      response = extract_text(base64_im0).choices[0].message.content\n",
        "\n",
        "      print(f\"Extracted text: {response}\")\n",
        "\n",
        "      ann.box_label(box, label=str(response), color=colors(cls, True))  # Draw the bounding boxes\n",
        "\n",
        "    video_writer.write(im0)   # Write the processed video frame\n",
        "\n",
        "cap.release() # Release the video capture\n",
        "video_writer.release()  # Release the video writer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img align=\"left\" src=\"https://github.com/user-attachments/assets/d9f5efe7-d058-48f4-9515-f7ccf6084c2f\" height=\"640\">"
      ],
      "metadata": {
        "id": "1X_dfSSH3ex5"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}