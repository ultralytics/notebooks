{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689-h7AurFgV",
   "metadata": {
    "id": "689-h7AurFgV"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
    "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
    "\n",
    "  [中文](https://docs.ultralytics.com/zh/) | [한국어](https://docs.ultralytics.com/ko/) | [日本語](https://docs.ultralytics.com/ja/) | [Русский](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Français](https://docs.ultralytics.com/fr/) | [Español](https://docs.ultralytics.com/es/) | [Português](https://docs.ultralytics.com/pt/) | [Türkçe](https://docs.ultralytics.com/tr/) | [Tiếng Việt](https://docs.ultralytics.com/vi/) | [العربية](https://docs.ultralytics.com/ar/)\n",
    "\n",
    "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
    "  <a href=\"https://colab.research.google.com/github/ultralytics/notebooks/blob/main/notebooks/how-to-use-google-gemini-models-for-object-detection-image-captioning-and-ocr.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "  \n",
    "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
    "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
    "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
    "  \n",
    "  This notebook demonstrates how to use <a href=\"https://ai.google.dev/gemini-api/docs/models\">Google Gemini models</a>, including the newly released Gemini 2.5 Pro (March 2025), with Ultralytics <a href=\"https://github.com/ultralytics/ultralytics\">YOLO</a> utilities for object detection, image segmentation, and generating visualizations from text prompts such as image captioning.\n",
    "  \n",
    "  We aim to provide resources that help you maximize the potential of the Gemini family. If you need assistance, feel free to raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> or join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for discussions and support!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4v_Bmj1rte4o",
   "metadata": {
    "id": "4v_Bmj1rte4o"
   },
   "source": [
    "# What is Google Gemini?\n",
    "\n",
    "Google Gemini is a family of multimodal AI models designed to help you process and understand various data types, including text, images, audio, video, and code. The suite includes both Large Language Models (LLMs) and Vision-Language Models (VLMs), enabling you to build versatile AI applications across domains.\n",
    "\n",
    "In March 2025, Google released `Gemini 2.5 Pro Experimental`, which brings enhanced reasoning capabilities, improved code generation, and stronger  multimodal understanding, making it a powerful tool for vision-based workflows.\n",
    "\n",
    "<img src=\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/gemini-2.5-pro-exp-benchmark.jpg\" alt=\"Gemini 2.5 Pro Experimental Benchmarks\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neKA5217uAUY",
   "metadata": {
    "id": "neKA5217uAUY"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To get started, we need to install the `ultralytics` and `google-genai` libraries. 🚀\n",
    "\n",
    "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
    "\n",
    "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "JsmwlsJZqvrV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsmwlsJZqvrV",
    "outputId": "012c1724-d556-4351-e6fd-7ca4220ec1e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q google-genai ultralytics\n",
    "\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import ultralytics\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from ultralytics.utils.downloads import safe_download\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DVi_F3Nd14Ng",
   "metadata": {
    "id": "DVi_F3Nd14Ng"
   },
   "source": [
    "## Inference function\n",
    "\n",
    "Let’s configure the Gemini client to accept an image and perform tasks based on your text prompts. Find more information about [Gemini models](https://ai.google.dev/gemini-api/docs/models). To get started, generate your API key by logging into <a href=\"https://aistudio.google.com/\">Google AI Studio</a>. 🚀\n",
    "\n",
    "The inference function will be used throughout the notebook to perform various operations using the Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "O4PFWjKyqYZd",
   "metadata": {
    "id": "O4PFWjKyqYZd"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gemini client with your API key\n",
    "client = genai.Client(api_key=\"api_key\")\n",
    "\n",
    "\n",
    "def inference(image, prompt, temp=0.5):\n",
    "    \"\"\"\n",
    "    Performs inference using Google Gemini 2.5 Pro Experimental model.\n",
    "\n",
    "    Args:\n",
    "        image (str or genai.types.Blob): The image input, either as a base64-encoded string or Blob object.\n",
    "        prompt (str): A text prompt to guide the model's response.\n",
    "        temp (float, optional): Sampling temperature for response randomness. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        str: The text response generated by the Gemini model based on the prompt and image.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-preview-05-20\",  # or \"gemini-2.5-pro-exp-03-25\"\n",
    "        contents=[prompt, image],  # Provide both the text prompt and image as input\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=temp,  # Controls creativity vs. determinism in output\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return response.text  # Return the generated textual response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5492d-83ad-44b1-a06b-4c55ecc8abb4",
   "metadata": {
    "id": "c6f5492d-83ad-44b1-a06b-4c55ecc8abb4"
   },
   "source": [
    "## Download and read the Image  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lZV5NXV7yUz8",
   "metadata": {
    "id": "lZV5NXV7yUz8"
   },
   "source": [
    "For testing, we'll fetch `gemini-image1.jpg` from [Ultralytics](https://ultralytics.com/) [notebooks assets](https://github.com/ultralytics/notebooks/releases/tag/v0.0.0) and use it for tasks like image captioning, object detection, image segmentation, and OCR. Feel free to use any image of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd464a8e-0f93-465e-a58e-ec69fe739db7",
   "metadata": {
    "id": "dd464a8e-0f93-465e-a58e-ec69fe739db7"
   },
   "outputs": [],
   "source": [
    "def read_image(filename=None):\n",
    "    if filename is not None:\n",
    "        image_name = filename\n",
    "    else:\n",
    "        image_name = \"bus.jpg\"  # or \"zidane.jpg\"\n",
    "\n",
    "    # Download the image\n",
    "    safe_download(f\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/{image_name}\")\n",
    "\n",
    "    # Read image with opencv\n",
    "    image = cv2.cvtColor(cv2.imread(f\"{image_name}\"), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Extract width and height\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # # Read the image using OpenCV and convert it into the PIL format\n",
    "    return Image.fromarray(image), w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V-tDSgnuHimF",
   "metadata": {
    "id": "V-tDSgnuHimF"
   },
   "source": [
    "![Input image for testing gemini-2.5-pro model](https://github.com/ultralytics/notebooks/releases/download/v0.0.0/gemini-inference-image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qqbda7-vFAPj",
   "metadata": {
    "id": "qqbda7-vFAPj"
   },
   "source": [
    "## Results formatting\n",
    "\n",
    "You can use this function to clean the raw string output by removing Markdown formatting (like ```json), so it can be safely parsed as JSON for bounding box extraction and plotting. 🧼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boA7R0yGE3dU",
   "metadata": {
    "id": "boA7R0yGE3dU"
   },
   "outputs": [],
   "source": [
    "def clean_results(results):\n",
    "    \"\"\"Clean the results for visualization.\"\"\"\n",
    "    return results.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bbeed-4f85-4f35-9223-80b7b4e38da1",
   "metadata": {
    "id": "230bbeed-4f85-4f35-9223-80b7b4e38da1"
   },
   "source": [
    "## Object detection\n",
    "\n",
    "Gemini models support object detection, helping you efficiently identify and recognize multiple objects within an image. 😀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a68a3-669d-423c-bd6c-29194ca16e37",
   "metadata": {
    "id": "252a68a3-669d-423c-bd6c-29194ca16e37"
   },
   "outputs": [],
   "source": [
    "# Define the text prompt\n",
    "prompt = \"\"\"\n",
    "Detect the 2d bounding boxes of objects in image.\n",
    "\"\"\"\n",
    "\n",
    "# Fixed, plotting function depends on this.\n",
    "output_prompt = \"Return just box_2d and labels, no additional text.\"\n",
    "\n",
    "image, w, h = read_image(\"gemini-image1.jpg\")  # Read img, extract width, height\n",
    "\n",
    "results = inference(image, prompt + output_prompt)  # Perform inference\n",
    "\n",
    "cln_results = json.loads(clean_results(results))  # Clean results, list convert\n",
    "\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "\n",
    "for idx, item in enumerate(cln_results):\n",
    "    # By default, gemini model return output with y coordinates first.\n",
    "    # Scale normalized box coordinates (0–1000) to image dimensions\n",
    "    y1, x1, y2, x2 = item[\"box_2d\"]  # bbox post processing,\n",
    "    y1 = y1 / 1000 * h\n",
    "    x1 = x1 / 1000 * w\n",
    "    y2 = y2 / 1000 * h\n",
    "    x2 = x2 / 1000 * w\n",
    "\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1  # Swap x-coordinates if needed\n",
    "    if y1 > y2:\n",
    "        y1, y2 = y2, y1  # Swap y-coordinates if needed\n",
    "\n",
    "    annotator.box_label([x1, y1, x2, y2], label=item[\"label\"], color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74O5bCOCWyVT",
   "metadata": {
    "id": "74O5bCOCWyVT"
   },
   "source": [
    "![Object detection with gemini-2.5-pro model](https://github.com/ultralytics/notebooks/releases/download/v0.0.0/gemini-inference-image-processed.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "htIbkTrYS1Rf",
   "metadata": {
    "id": "htIbkTrYS1Rf"
   },
   "source": [
    "## Reasoning capabilities\n",
    "\n",
    "With Gemini models, you can tackle complex tasks using advanced reasoning that understands context and delivers more precise results. 🧠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6HL0Av3RTZqQ",
   "metadata": {
    "id": "6HL0Av3RTZqQ"
   },
   "outputs": [],
   "source": [
    "# Define the text prompt\n",
    "prompt = \"\"\"\n",
    "Detect the 2d bounding box around:\n",
    "highlight the area of morning light +\n",
    "notebook on PC table\n",
    "potted plant near mirror.\n",
    "\"\"\"\n",
    "\n",
    "# Fixed, plotting function depends on this.\n",
    "output_prompt = \"Return just box_2d and labels, no additional text.\"\n",
    "\n",
    "image, w, h = read_image(\"gemini-image2.jpg\")  # Read image and extract width, height\n",
    "\n",
    "results = inference(image, prompt + output_prompt)\n",
    "\n",
    "# Clean the results and load results in list format\n",
    "cln_results = json.loads(clean_results(results))\n",
    "\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "\n",
    "for idx, item in enumerate(cln_results):\n",
    "    # By default, gemini model return output with y coordinates first.\n",
    "    # Scale normalized box coordinates (0–1000) to image dimensions\n",
    "    y1, x1, y2, x2 = item[\"box_2d\"]  # bbox post processing,\n",
    "    y1 = y1 / 1000 * h\n",
    "    x1 = x1 / 1000 * w\n",
    "    y2 = y2 / 1000 * h\n",
    "    x2 = x2 / 1000 * w\n",
    "\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1  # Swap x-coordinates if needed\n",
    "    if y1 > y2:\n",
    "        y1, y2 = y2, y1  # Swap y-coordinates if needed\n",
    "\n",
    "    annotator.box_label([x1, y1, x2, y2], label=item[\"label\"], color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pj95iN4FVTyp",
   "metadata": {
    "id": "pj95iN4FVTyp"
   },
   "source": [
    "<img src=\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/gemini-inference-image-reasoning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbea76d-1b31-45b4-b3d9-ea8ada73bd7c",
   "metadata": {
    "id": "6cbea76d-1b31-45b4-b3d9-ea8ada73bd7c"
   },
   "source": [
    "## Image captioning  \n",
    "\n",
    "You can use Gemini models for image captioning to generate meaningful text descriptions that summarize the content of an image. 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a0804b7f-c6a3-44c5-a5cf-0493e070a3e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0804b7f-c6a3-44c5-a5cf-0493e070a3e1",
    "outputId": "63196ce5-294d-4212-dfb4-fa7d96079f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/gemini-image4.jpg to 'gemini-image4.jpg'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267k/267k [00:00<00:00, 12.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunlight spilled across the wooden desk, illuminating the quiet workspace.\n",
      "A laptop sat open, flanked by a steaming red mug and a waiting tablet.\n",
      "Nearby, a notebook held potential ideas, bathed in the warm morning glow.\n",
      "Potted plants added touches of green, bringing the outside indoors.\n",
      "It was a peaceful setup, ready for a day of focused creativity.\n"
     ]
    }
   ],
   "source": [
    "# Define the text prompt\n",
    "prompt = \"\"\"\n",
    "What's inside the image, generate a detailed captioning in the form of short\n",
    "story, Make 4-5 lines and start each sentence on a new line.\n",
    "\"\"\"\n",
    "\n",
    "image, _, _ = read_image(\"gemini-image4.jpg\")  # Read image and extract width, height\n",
    "\n",
    "print(inference(image, prompt))  # Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CXImtrYoaJHC",
   "metadata": {
    "id": "CXImtrYoaJHC"
   },
   "source": [
    "<img src=\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/gemini-inference-image-captioning.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27z21nwiargQ",
   "metadata": {
    "id": "27z21nwiargQ"
   },
   "source": [
    "## OCR\n",
    "\n",
    "Gemini models also support Optical Character Recognition (OCR), helping you detect and extract text from images with speed and accuracy. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-vsUgD3-bwOK",
   "metadata": {
    "id": "-vsUgD3-bwOK"
   },
   "outputs": [],
   "source": [
    "# Define the text prompt\n",
    "prompt = \"\"\"\n",
    "Extract the text from the image\n",
    "\"\"\"\n",
    "\n",
    "# Fixed, plotting function depends on this.\n",
    "output_prompt = \"\"\"\n",
    "Return just box_2d which will be location of detected text areas + label\"\"\"\n",
    "\n",
    "image, w, h = read_image(\"gemini-image3.png\")  # Read image and extract width, height\n",
    "\n",
    "results = inference(image, prompt + output_prompt)\n",
    "\n",
    "# Clean the results and load results in list format\n",
    "cln_results = json.loads(clean_results(results))\n",
    "\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "\n",
    "for idx, item in enumerate(cln_results):\n",
    "    # By default, gemini model return output with y coordinates first.\n",
    "    # Scale normalized box coordinates (0–1000) to image dimensions\n",
    "    y1, x1, y2, x2 = item[\"box_2d\"]  # bbox post processing,\n",
    "    y1 = y1 / 1000 * h\n",
    "    x1 = x1 / 1000 * w\n",
    "    y2 = y2 / 1000 * h\n",
    "    x2 = x2 / 1000 * w\n",
    "\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1  # Swap x-coordinates if needed\n",
    "    if y1 > y2:\n",
    "        y1, y2 = y2, y1  # Swap y-coordinates if needed\n",
    "\n",
    "    annotator.box_label([x1, y1, x2, y2], label=item[\"label\"], color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zSa2nQuUkXvt",
   "metadata": {
    "id": "zSa2nQuUkXvt"
   },
   "source": [
    "![OCR with gemini-2.5-pro model](https://github.com/ultralytics/notebooks/releases/download/v0.0.0/gemini-inference-image-ocr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sn6bwC-QUbd7",
   "metadata": {
    "id": "sn6bwC-QUbd7"
   },
   "source": [
    "## Additional Resources  \n",
    "\n",
    "✅ Learn more about Gemini 2.5: [here](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)  \n",
    "✅ Ultralytics Annotator: [here](https://docs.ultralytics.com/reference/utils/plotting/)\n",
    "\n",
    "🌟 Explore the [Ultralytics Notebooks](https://github.com/ultralytics/notebooks/) and give them a star to boost your AI journey! 🚀\n",
    "\n",
    "Built with 💙 by [Ultralytics](https://ultralytics.com/)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
