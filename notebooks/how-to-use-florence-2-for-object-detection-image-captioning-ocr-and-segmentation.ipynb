{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689-h7AurFgV",
   "metadata": {
    "id": "689-h7AurFgV"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
    "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\n",
    "\n",
    "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
    "\n",
    "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
    "  <a href=\"https://colab.research.google.com/github/ultralytics/notebooks/blob/main/notebooks/how-to-use-florence-2-for-object-detection-image-captioning-ocr-and-segmentation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "  \n",
    "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
    "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
    "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
    "  \n",
    "  This notebook demonstrates how to use Florence 2 with Ultralytics <a href=\"https://github.com/ultralytics/ultralytics\">YOLO</a> annotators for object detection, image segmentation, and generating visualizations from text prompts, such as image captioning. We aim to provide resources that help you maximize the potential of Florence-2. If you need assistance, feel free to raise an issue on <a href=\"https://github.com/ultralytics/ultralytics\">GitHub</a> or join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for discussions and support!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4v_Bmj1rte4o",
   "metadata": {
    "id": "4v_Bmj1rte4o"
   },
   "source": [
    "# What is Florence-2?\n",
    "\n",
    "Microsoft released the Florence-2 model last year, It is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. It can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation.\n",
    "  \n",
    "It leverages the FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\n",
    "<img src=\"https://github.com/user-attachments/assets/28e24618-9777-4dda-af81-8be730993976\" alt=\"Florence 2 Architecture\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neKA5217uAUY",
   "metadata": {
    "id": "neKA5217uAUY"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To get started, we need to install the `ultralytics` and `transformers` libraries. üöÄ\n",
    "\n",
    "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
    "\n",
    "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "WHmtXrUvp3Zn",
   "metadata": {
    "collapsed": true,
    "id": "WHmtXrUvp3Zn"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.49.0 ultralytics\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import ultralytics\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from ultralytics.solutions.solutions import SolutionAnnotator\n",
    "from ultralytics.utils.downloads import safe_download\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rwhmtntwuqTY",
   "metadata": {
    "id": "rwhmtntwuqTY"
   },
   "source": [
    "## Download Florence-2 model\n",
    "\n",
    "We'll utilize the `transformers` library to fetch the `Florence-2-large` model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b356b-630d-4b89-8139-1995e31822e7",
   "metadata": {
    "id": "998b356b-630d-4b89-8139-1995e31822e7"
   },
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Florence-2-large\"\n",
    "\n",
    "# Ensure the runtime is set to GPU in Colab.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=\"auto\").eval().cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d43102-ba85-4438-b971-063d8677129b",
   "metadata": {
    "id": "54d43102-ba85-4438-b971-063d8677129b"
   },
   "source": [
    "## Inference function  \n",
    "\n",
    "In this notebook, we'll explore various operations using the Florence-2 model. Here, we'll define an inference function capable of handling image captioning, object detection, image segmentation, and OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5427f95b-3c6e-4834-b08f-8af1a38306b7",
   "metadata": {
    "id": "5427f95b-3c6e-4834-b08f-8af1a38306b7"
   },
   "outputs": [],
   "source": [
    "def inference(image, task_prompt, text_input=None):\n",
    "    \"\"\"\n",
    "    Performs inference using the given image and task prompt.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image or tensor): The input image for processing.\n",
    "        task_prompt (str): The prompt specifying the task for the model.\n",
    "        text_input (str, optional): Additional text input to refine the task prompt.\n",
    "\n",
    "    Returns:\n",
    "        dict: The model's processed response after inference.\n",
    "    \"\"\"\n",
    "    # Combine task prompt with additional text input if provided\n",
    "    prompt = task_prompt if text_input is None else task_prompt + text_input\n",
    "\n",
    "    # Generate the input data for model processing from the given prompt and image\n",
    "    inputs = processor(\n",
    "        text=prompt,  # Text input for the model\n",
    "        images=image,  # Image input for the model\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    ).to(\"cuda\", torch.float16)  # Move inputs to GPU with float16 precision\n",
    "\n",
    "    # Generate model predictions (token IDs)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].cuda(),  # Convert text input IDs to CUDA\n",
    "        pixel_values=inputs[\"pixel_values\"].cuda(),  # Convert image pixel values to CUDA\n",
    "        max_new_tokens=1024,  # Set maximum number of tokens to generate\n",
    "        early_stopping=False,  # Disable early stopping\n",
    "        do_sample=False,  # Use deterministic inference\n",
    "        num_beams=3,  # Set beam search width for better predictions\n",
    "    )\n",
    "\n",
    "    # Decode generated token IDs into text\n",
    "    generated_text = processor.batch_decode(\n",
    "        generated_ids,  # Generated token IDs\n",
    "        skip_special_tokens=False,  # Retain special tokens in output\n",
    "    )[0]  # Extract first result from batch\n",
    "\n",
    "    # Post-process the generated text into a structured response\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,  # Raw generated text\n",
    "        task=task_prompt,  # Task type for post-processing\n",
    "        image_size=(image.width, image.height),  # Original image dimensions for scaling output\n",
    "    )\n",
    "\n",
    "    return parsed_answer  # Return the final processed output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5492d-83ad-44b1-a06b-4c55ecc8abb4",
   "metadata": {
    "id": "c6f5492d-83ad-44b1-a06b-4c55ecc8abb4"
   },
   "source": [
    "## Download and read the Image  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lZV5NXV7yUz8",
   "metadata": {
    "id": "lZV5NXV7yUz8"
   },
   "source": [
    "For testing, we'll fetch `bus.jpg` from [Ultralytics](https://ultralytics.com/) assets and use it for tasks like image captioning, object detection, image segmentation, and OCR. Feel free to use any image of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd464a8e-0f93-465e-a58e-ec69fe739db7",
   "metadata": {
    "id": "dd464a8e-0f93-465e-a58e-ec69fe739db7"
   },
   "outputs": [],
   "source": [
    "def read_image(filename=None):\n",
    "    if filename is not None:\n",
    "        image_name = filename\n",
    "    else:\n",
    "        image_name = \"bus.jpg\"  # or \"zidane.jpg\"\n",
    "\n",
    "    # Download the image\n",
    "    safe_download(f\"https://github.com/ultralytics/assets/releases/download/v0.0.0/{image_name}\")\n",
    "\n",
    "    # Read the image using OpenCV and convert it into the PIL format\n",
    "    return Image.fromarray(cv2.cvtColor(cv2.imread(f\"/content/{image_name}\"), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V-tDSgnuHimF",
   "metadata": {
    "id": "V-tDSgnuHimF"
   },
   "source": [
    "<img src=\"https://github.com/user-attachments/assets/e3f77be3-4174-4310-bd33-d2955cda7a94\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2530935-e270-44d2-bb38-5b2b88ebb0df",
   "metadata": {
    "id": "a2530935-e270-44d2-bb38-5b2b88ebb0df"
   },
   "source": [
    "## Execute pre-defined tasks without extra inputs\n",
    "\n",
    "With the model loaded into memory, the input image ready for processing, and functions in place for preprocessing and post-processing, it's time to play some magic with Florence-2. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bbeed-4f85-4f35-9223-80b7b4e38da1",
   "metadata": {
    "id": "230bbeed-4f85-4f35-9223-80b7b4e38da1"
   },
   "source": [
    "### Object detection  \n",
    "\n",
    "Florence-2 supports object detection, allowing us to efficiently identify and recognize various objects within an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a68a3-669d-423c-bd6c-29194ca16e37",
   "metadata": {
    "id": "252a68a3-669d-423c-bd6c-29194ca16e37"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Result format üòÄ\n",
    "{\n",
    "    \"<OD>\": {\n",
    "        \"bboxes\": [[x1, y1, x2, y2], ...],\n",
    "        \"labels\": [\"label1\", \"label2\", ...]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = \"<OD>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(image, task_prompt)[\"<OD>\"]\n",
    "\n",
    "# Plot the results on an image\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "\n",
    "for idx, (box, label) in enumerate(zip(results[\"bboxes\"], results[\"labels\"])):\n",
    "    annotator.box_label(box, label=label, color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74O5bCOCWyVT",
   "metadata": {
    "id": "74O5bCOCWyVT"
   },
   "source": [
    "![Object detection using florence-2](https://github.com/user-attachments/assets/6bf6820b-8eb5-44f2-a626-64d76c92b981)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bWHF-3WpEgl0",
   "metadata": {
    "id": "bWHF-3WpEgl0"
   },
   "source": [
    "### Object detection (video)\n",
    "\n",
    "You can also run videos through the Florence-2 model‚Äîyep, not just images. Frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bu2F91psD1FX",
   "metadata": {
    "id": "bu2F91psD1FX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Result format üòÄ\n",
    "{\n",
    "    \"<OD>\": {\n",
    "        \"bboxes\": [[x1, y1, x2, y2], ...],\n",
    "        \"labels\": [\"label1\", \"label2\", ...]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "labels_dict = {}\n",
    "task_prompt = \"<OD>\"\n",
    "\n",
    "# Download and read the demo video file\n",
    "safe_download(\"https://github.com/ultralytics/assets/releases/download/v0.0.0/solutions_ci_demo.mp4\")\n",
    "cap = cv2.VideoCapture(\"/content/solutions_ci_demo.mp4\")\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"florence-2-output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        results = inference(image, task_prompt=task_prompt)[task_prompt]\n",
    "\n",
    "        annotator = Annotator(image, line_width=6)  # initialize annotator\n",
    "\n",
    "        for idx, (box, label) in enumerate(zip(results[\"bboxes\"], results[\"labels\"])):\n",
    "            x1, y1, x2, y2 = box\n",
    "            if x1 > x2:\n",
    "                x1, x2 = x2, x1  # Swap x-coordinates if needed\n",
    "            if y1 > y2:\n",
    "                y1, y2 = y2, y1  # Swap y-coordinates if needed\n",
    "\n",
    "            # Add label color\n",
    "            if label not in labels_dict:\n",
    "                color = colors(random.randint(1, 20))\n",
    "                labels_dict[label] = color\n",
    "            else:\n",
    "                color = labels_dict[label]\n",
    "\n",
    "            # Box plotting\n",
    "            annotator.box_label([x1, y1, x2, y2], label=label, color=color)\n",
    "\n",
    "        video_writer.write(cv2.cvtColor(annotator.result(), cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbea76d-1b31-45b4-b3d9-ea8ada73bd7c",
   "metadata": {
    "id": "6cbea76d-1b31-45b4-b3d9-ea8ada73bd7c"
   },
   "source": [
    "### Caption  \n",
    "\n",
    "Generate captions for an input image without providing a custom prompt. By default, Caption feature in Florence-2 provides three levels of captions:\n",
    "\n",
    "- `CAPTION`\n",
    "- `DETAILED_CAPTION`\n",
    "- `MORE_DETAILED_CAPTION`.  \n",
    "\n",
    "üöÄ `CAPTION` is generally more accurate, while `DETAILED_CAPTION` and `MORE_DETAILED_CAPTION` may not always be as precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0804b7f-c6a3-44c5-a5cf-0493e070a3e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0804b7f-c6a3-44c5-a5cf-0493e070a3e1",
    "outputId": "ed38c77e-eee3-497d-9fed-808f05d5629f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<CAPTION>': 'A group of people walking down a street next to a bus.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_prompt = \"<CAPTION>\"\n",
    "inference(image, task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11700ca3-54dc-4de0-836e-9814cee3a3d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11700ca3-54dc-4de0-836e-9814cee3a3d2",
    "outputId": "d84dfca7-b5fa-44a8-d2c6-e85a9fc5325b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<DETAILED_CAPTION>': 'The image shows a group of people walking down a street next to a bus, with a building in the background featuring windows, railings, doors, and balconies, as well as trees and a sign board. The people are wearing different types of clothing, suggesting that the image is related to the use of augmented reality in Madrid.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_prompt = \"<DETAILED_CAPTION>\"\n",
    "inference(image, task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a8ec2-447f-400d-b545-d0577b6bdbc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "005a8ec2-447f-400d-b545-d0577b6bdbc9",
    "outputId": "592c8bb9-1c5a-4553-b195-dad8c7ee7f0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<MORE_DETAILED_CAPTION>': 'The image shows a blue and white bus on a street with a group of people walking on the sidewalk in front of it. The bus has the words \"man\" and \"trousers\" written on it, along with a logo of a leaf and the text \"cero emisiones\" which translates to \"man\". There are also several labels on the bus, including \"coat\", \"trouser\", \"footwear\", \"wheel\", and \"focwear\". The people in the image appear to be of different ages and genders, and they are dressed in casual clothing. The street is lined with buildings and there is a tree on the left side of the image.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "inference(image, task_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50558829-44ed-4dec-8512-5ba12efb0bdd",
   "metadata": {
    "id": "50558829-44ed-4dec-8512-5ba12efb0bdd"
   },
   "source": [
    "### Dense region caption\n",
    "\n",
    "For more precise results, such as specific locations, cities, places, or object types, we can use dense region captioning. Instead of a general caption like `\"a bus in an image,\"` we can obtain a more detailed description, such as `\"an electric bus in Madrid, Spain.\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326bf03-51c8-4784-ae02-3d32ab97fda8",
   "metadata": {
    "id": "a326bf03-51c8-4784-ae02-3d32ab97fda8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Result format üòÄ\n",
    "{\n",
    "    \"<DENSE_REGION_CAPTION>\": {\n",
    "        \"bboxes\": [[x1, y1, x2, y2], ...],\n",
    "        \"labels\": [\"label1\", \"label2\", ...]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(image, task_prompt)[\"<DENSE_REGION_CAPTION>\"]\n",
    "\n",
    "# Plot the results on an image\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "\n",
    "for idx, (box, label) in enumerate(zip(results[\"bboxes\"], results[\"labels\"])):\n",
    "    annotator.box_label(box, label=label, color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WQohU8vBW-uk",
   "metadata": {
    "id": "WQohU8vBW-uk"
   },
   "source": [
    "![Dense region captioning using florence-2](https://github.com/user-attachments/assets/b10efb49-f8a8-46f6-aa62-36782d824d9f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea7184-03da-4ede-b1ef-ba458ef2fdd3",
   "metadata": {
    "id": "18ea7184-03da-4ede-b1ef-ba458ef2fdd3"
   },
   "source": [
    "### Region proposal\n",
    "\n",
    "Region proposal refers to the process of identifying areas in an image that are likely to contain objects. It helps in object detection by suggesting potential object locations before classification and bounding box refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26e9a9-8662-491e-a6da-0d9ad3a4a71c",
   "metadata": {
    "id": "bd26e9a9-8662-491e-a6da-0d9ad3a4a71c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Results format üéâ\n",
    "{\n",
    "    \"<REGION_PROPOSAL>\": {\n",
    "        \"bboxes\": [[x1, y1, x2, y2], ...],\n",
    "        \"labels\": [\"label1\", \"label2\", ...]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = \"<REGION_PROPOSAL>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(image, task_prompt)[\"<REGION_PROPOSAL>\"]\n",
    "\n",
    "# Plot the results on an image\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "for idx, box in enumerate(results[\"bboxes\"]):\n",
    "    x1, y1, x2, y2 = box\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1  # Swap x-coordinates if needed\n",
    "    if y1 > y2:\n",
    "        y1, y2 = y2, y1  # Swap y-coordinates if needed\n",
    "\n",
    "    # Update the bounding box correctly\n",
    "    annotator.box_label([x1, y1, x2, y2], color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXJM3QQ0XUoT",
   "metadata": {
    "id": "xXJM3QQ0XUoT"
   },
   "source": [
    "![Region proposal using florence-2](https://github.com/user-attachments/assets/b0e3e004-f9f6-4937-9ea9-431be56e1476)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d31fa-20cf-470d-af1b-a125fc46f9e7",
   "metadata": {
    "id": "c25d31fa-20cf-470d-af1b-a125fc46f9e7"
   },
   "source": [
    "## Execute pre-defined tasks that require additional inputs\n",
    "\n",
    "Here, we can provide a custom prompt to generate precise results tailored to specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f342e2c-4fd0-4705-876c-198bd3320ab6",
   "metadata": {
    "id": "3f342e2c-4fd0-4705-876c-198bd3320ab6"
   },
   "source": [
    "### Phrase grounding\n",
    "\n",
    "We can also extract specific results i.e. `a bus` or `Madrid bus` by passing additional text_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a5f79-9290-4d1a-bf4f-16f1b5aaf197",
   "metadata": {
    "id": "186a5f79-9290-4d1a-bf4f-16f1b5aaf197"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Results format: üöÄ\n",
    "{\n",
    "    \"<CAPTION_TO_PHRASE_GROUNDING>\": {\n",
    "        \"bboxes\": [[x1, y1, x2, y2], ...],\n",
    "        \"labels\": [\"label1\", \"label2\", ...]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(image, task_prompt, text_input=\"a madrid bus, glasses\")[\"<CAPTION_TO_PHRASE_GROUNDING>\"]\n",
    "\n",
    "# Plot the results on an image\n",
    "annotator = Annotator(image)  # initialize Ultralytics annotator\n",
    "\n",
    "for idx, (box, label) in enumerate(zip(results[\"bboxes\"], results[\"labels\"])):\n",
    "    annotator.box_label(box, label=label, color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oMeYPU_nXlhk",
   "metadata": {
    "id": "oMeYPU_nXlhk"
   },
   "source": [
    "![Captions to phrase grouding using florence-2](https://github.com/user-attachments/assets/4c2dee5b-afd2-4d99-bc8d-bf24b93937fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd75b13-b113-4a4b-acea-5a8562a10ed2",
   "metadata": {
    "id": "5bd75b13-b113-4a4b-acea-5a8562a10ed2"
   },
   "source": [
    "### Referring to expression segmentation\n",
    "\n",
    "We can use Florence-2 for segmentation based on specific expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6411cf-f46f-4eab-9476-9a5641a75a46",
   "metadata": {
    "id": "7a6411cf-f46f-4eab-9476-9a5641a75a46"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Results format üòÉ\n",
    "\n",
    "# one object is represented by a list of polygons. each polygon is [x1, y1, x2, y2, ..., xn, yn]\n",
    "{\n",
    "    \"<REFERRING_EXPRESSION_SEGMENTATION>\": {\n",
    "        {'Polygons': [[[polygon]], ...],\n",
    "        \"labels\": [\"label1\", \"label2\", ...]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(image, task_prompt, text_input=\"person with black coat showoff\")[\n",
    "    \"<REFERRING_EXPRESSION_SEGMENTATION>\"\n",
    "]\n",
    "\n",
    "# segmentation mask function required the numpy array image, not PIL based image.\n",
    "annotator = SolutionAnnotator(np.array(image, dtype=np.uint8))\n",
    "\n",
    "for idx, (polygons, label) in enumerate(zip(results[\"polygons\"], results[\"labels\"])):\n",
    "    for polygon in polygons:\n",
    "        polygon = np.array(polygon).reshape(-1, 2)\n",
    "\n",
    "        if len(polygon) < 3:\n",
    "            print(\"Invalid polygon:\", polygon)\n",
    "            continue\n",
    "\n",
    "        annotator.segmentation_mask(polygon, label=label, mask_color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X0MIO0KbXwfE",
   "metadata": {
    "id": "X0MIO0KbXwfE"
   },
   "source": [
    "![Captions to phrase grouding using florence-2](https://github.com/user-attachments/assets/d70a8c3a-709d-43cf-863a-e2c16036424e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed56632-9052-4c4e-9426-f8538889cb6a",
   "metadata": {
    "id": "eed56632-9052-4c4e-9426-f8538889cb6a"
   },
   "source": [
    "### Region to segmentation\n",
    "\n",
    "Here, we can segment a specific area by providing its coordinates.\n",
    "\n",
    "`<loc_x1>\\<loc_y1>\\<loc_x2>\\<loc_y2>, [x1, y1, x2, y2] is the normalized coordinates in [0, 999].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed60a42-78ff-455c-b0d6-a3e25c0d801d",
   "metadata": {
    "id": "bed60a42-78ff-455c-b0d6-a3e25c0d801d"
   },
   "outputs": [],
   "source": [
    "task_prompt = \"<REGION_TO_SEGMENTATION>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(\n",
    "    image, task_prompt, text_input=\"[3.8328723907470703, 229.35601806640625,796.2098999023438, 728.4313354492188]\"\n",
    ")[\"<REGION_TO_SEGMENTATION>\"]\n",
    "\n",
    "# segmentation mask function required the numpy array image, not PIL based image.\n",
    "annotator = SolutionAnnotator(np.array(image, dtype=np.uint8))\n",
    "\n",
    "for idx, (polygons, label) in enumerate(zip(results[\"polygons\"], results[\"labels\"])):\n",
    "    for _polygon in polygons:\n",
    "        _polygon = np.array(_polygon).reshape(-1, 2)\n",
    "\n",
    "        if len(_polygon) < 3:\n",
    "            print(\"Invalid polygon:\", _polygon)\n",
    "            continue\n",
    "\n",
    "        annotator.segmentation_mask(_polygon, label=label, mask_color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyDQfElwX6r8",
   "metadata": {
    "id": "wyDQfElwX6r8"
   },
   "source": [
    "![Region to segmentation using florence-2](https://github.com/user-attachments/assets/0dc7aff0-a688-4a08-a895-ba05678a2dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accf12e-3441-4d3e-86e3-d5755601d7ce",
   "metadata": {
    "id": "3accf12e-3441-4d3e-86e3-d5755601d7ce"
   },
   "source": [
    "### Open vocabulary detection\n",
    "\n",
    "It enables models to detect both objects and text (OCR) without being limited to predefined categories, making it highly adaptable for real-world applications like retail, navigation, and document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43333b69-5484-4c16-b3cf-331d74c36780",
   "metadata": {
    "id": "43333b69-5484-4c16-b3cf-331d74c36780"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Results format üòÄ\n",
    "\n",
    "{ '<OPEN_VOCABULARY_DETECTION>':\n",
    "\n",
    "      {'bboxes': [[x1, y1, x2, y2], [x1, y1, x2, y2], ...]],\n",
    "      'bboxes_labels': ['label_1', 'label_2', ..],\n",
    "      'polygons': [[[x1, y1, x2, y2, ..., xn, yn], [x1, y1, ..., xn, yn]], ...],\n",
    "      'polygons_labels': ['label_1', 'label_2', ...]\n",
    "      }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = \"<OPEN_VOCABULARY_DETECTION>\"\n",
    "image = read_image()\n",
    "\n",
    "results = inference(image, task_prompt, text_input=\"trees\")[\"<OPEN_VOCABULARY_DETECTION>\"]\n",
    "\n",
    "# segmentation mask function required the numpy array image, not PIL based image.\n",
    "annotator = Annotator(image)\n",
    "\n",
    "for idx, (box, label) in enumerate(zip(results[\"bboxes\"], results[\"bboxes_labels\"])):\n",
    "    annotator.box_label(box, color=colors(idx, True), label=label)\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9GENxU_EYFaj",
   "metadata": {
    "id": "9GENxU_EYFaj"
   },
   "source": [
    "![Open vocabulary detection using florence-2](https://github.com/user-attachments/assets/6368ac07-6083-4762-9020-b516be328d2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee1c41-0e99-41e8-b647-373cf84aa6a7",
   "metadata": {
    "id": "1dee1c41-0e99-41e8-b647-373cf84aa6a7"
   },
   "source": [
    "## OCR related tasks\n",
    "\n",
    "Florence 2 can also be utilized for OCR-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996738d-e8ab-4223-ad61-cb0f97658f74",
   "metadata": {
    "id": "b996738d-e8ab-4223-ad61-cb0f97658f74"
   },
   "outputs": [],
   "source": [
    "# Download and read the sample image for OCR\n",
    "# Sample images: ocr_img_for_florence_2_notebook.jpg, ocr_img_2_for_florence_2_notebook.jpg\n",
    "image = read_image(filename=\"ocr_img_1_for_florence_2_notebook.jpg\")\n",
    "\n",
    "task_prompt = \"<OCR_WITH_REGION>\"\n",
    "results = inference(image, task_prompt)[\"<OCR_WITH_REGION>\"]\n",
    "\n",
    "# segmentation mask function required the numpy array image, not PIL based image.\n",
    "annotator = Annotator(image, line_width=2)\n",
    "\n",
    "for idx, (box, label) in enumerate(zip(results[\"quad_boxes\"], results[\"labels\"])):\n",
    "    box = box[:4]\n",
    "    x1, y1, x2, y2 = box\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1  # Swap x-coordinates if needed\n",
    "    if y1 > y2:\n",
    "        y1, y2 = y2, y1  # Swap y-coordinates if needed\n",
    "\n",
    "    # Update the bounding box correctly\n",
    "    annotator.box_label([x1, y1, x2, y2], label=label, color=colors(idx, True))\n",
    "\n",
    "Image.fromarray(annotator.result())  # display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6BJAZDEGYN-M",
   "metadata": {
    "id": "6BJAZDEGYN-M"
   },
   "source": [
    "![Open vocabulary detection using florence-2](https://github.com/user-attachments/assets/1208bf8d-989a-4260-841b-f126ff69f061)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sn6bwC-QUbd7",
   "metadata": {
    "id": "sn6bwC-QUbd7"
   },
   "source": [
    "## Additional Resources  \n",
    "\n",
    "‚úÖ Florence-2 research paper: [here](https://arxiv.org/abs/2311.06242)  \n",
    "‚úÖ Ultralytics Annotator: [here](https://docs.ultralytics.com/reference/utils/plotting/)\n",
    "\n",
    "üåü Explore the [Ultralytics Notebooks](https://github.com/ultralytics/notebooks/) and give them a star to boost your AI journey! üöÄ\n",
    "\n",
    "Built with üíô by [Ultralytics](https://ultralytics.com/)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
